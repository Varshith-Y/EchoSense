{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 1: Speech To Braille"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import wave\n",
    "import pyaudio\n",
    "import whisper\n",
    "from autogen_agentchat.agents import AssistantAgent\n",
    "import os\n",
    "import wave\n",
    "import pyaudio\n",
    "import whisper\n",
    "from autogen import ConversableAgent  \n",
    "\n",
    "# Set your OpenAI API key (replace with your actual key)\n",
    "OPENAI_API_KEY = 'Enter Your OPENAI API Key Here'\n",
    "\n",
    "# Check if the API key is provided; if not, raise an error.\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    raise ValueError(\"Error: OPENAI_API_KEY not found in environment variables.\")\n",
    "#!/usr/bin/env python3\n",
    "\n",
    "# Create a ConversableAgent instance with the desired configuration.\n",
    "agent = ConversableAgent(\n",
    "    name=\"BrailleChatbot\",\n",
    "    llm_config={\n",
    "        \"config_list\": [{\"model\": \"gpt-4o-mini\", \"api_key\": OPENAI_API_KEY}]\n",
    "    },\n",
    "    code_execution_config=False,  \n",
    "    human_input_mode=\"NEVER\",  \n",
    ")\n",
    "\n",
    "# Define a dictionary mapping regular characters to their Braille equivalents.\n",
    "\n",
    "BRAILLE_DICT = {\n",
    "    'a': '⠁', 'b': '⠃', 'c': '⠉', 'd': '⠙', 'e': '⠑',\n",
    "    'f': '⠋', 'g': '⠛', 'h': '⠓', 'i': '⠊', 'j': '⠚',\n",
    "    'k': '⠅', 'l': '⠇', 'm': '⠍', 'n': '⠝', 'o': '⠕',\n",
    "    'p': '⠏', 'q': '⠟', 'r': '⠗', 's': '⠎', 't': '⠞',\n",
    "    'u': '⠥', 'v': '⠧', 'w': '⠺', 'x': '⠭', 'y': '⠽',\n",
    "    'z': '⠵',\n",
    "    ' ': ' ',\n",
    "    ',': '⠂', '.': '⠲', '?': '⠦', '!': '⠖', ';': '⠆',\n",
    "    ':': '⠒', '-': '⠤', '(': '⠣', ')': '⠜', '\"': '⠐⠦',\n",
    "    \"'\": '⠄',\n",
    "    '0': '⠼⠚', '1': '⠼⠁', '2': '⠼⠃', '3': '⠼⠉', '4': '⠼⠙',\n",
    "    '5': '⠼⠑', '6': '⠼⠋', '7': '⠼⠛', '8': '⠼⠓', '9': '⠼⠊'\n",
    "}\n",
    "\n",
    "def text_to_braille(text: str) -> str:\n",
    "    return ''.join(BRAILLE_DICT.get(char, '?') for char in text.lower())\n",
    "\n",
    "def record_audio(filename=\"temp.wav\", record_secs=5, rate=16000):\n",
    "    chunk = 1024\n",
    "    pa = pyaudio.PyAudio()\n",
    "    stream = pa.open(format=pyaudio.paInt16,\n",
    "                     channels=1,\n",
    "                     rate=rate,\n",
    "                     input=True,\n",
    "                     frames_per_buffer=chunk)\n",
    "    \n",
    "    print(\"Recording...\")\n",
    "\n",
    "    # Read audio chunks for the duration of record_secs\n",
    "\n",
    "    frames = [stream.read(chunk) for _ in range(int(rate / chunk * record_secs))]\n",
    "    print(\"Finished recording.\")\n",
    "\n",
    "    # Stop and close the audio stream\n",
    "    stream.stop_stream()\n",
    "    stream.close()\n",
    "    pa.terminate()\n",
    "    \n",
    "    # Save the recorded frames as a WAV file\n",
    "    with wave.open(filename, 'wb') as wf:\n",
    "        wf.setnchannels(1)\n",
    "        wf.setsampwidth(pa.get_sample_size(pyaudio.paInt16))\n",
    "        wf.setframerate(rate)\n",
    "        wf.writeframes(b''.join(frames))\n",
    "\n",
    "# Define the temporary WAV file name\n",
    "def main():\n",
    "    wav_file = \"temp.wav\"\n",
    "    record_audio(filename=wav_file, record_secs=8)\n",
    "\n",
    "    # Load the Whisper model for transcription\n",
    "    print(\"Loading Whisper model...\")\n",
    "    whisper_model = whisper.load_model(\"small\")\n",
    "\n",
    "    # Transcribe the recorded audio\n",
    "    print(\"Transcribing audio...\")\n",
    "    result = whisper_model.transcribe(wav_file)\n",
    "    user_text = result[\"text\"].strip()\n",
    "    print(f\"User said: {user_text}\")\n",
    "\n",
    "    # Remove the temporary audio file\n",
    "    os.remove(wav_file)\n",
    "\n",
    "    # Generate AI response using the ConversableAgent\n",
    "    print(\"AI thinking...\")\n",
    "    ai_response = agent.generate_reply(messages=[{\"role\": \"user\", \"content\": user_text}])\n",
    "\n",
    "    # Fallback response if no AI response was generated\n",
    "    ai_text = ai_response if ai_response else \"I couldn't generate a response.\"\n",
    "    print(f\"AI Response: {ai_text}\")\n",
    "\n",
    "    # Convert the AI response to Braille representation\n",
    "    braille_output = text_to_braille(ai_text)\n",
    "    print(f\"Braille Output: {braille_output}\")\n",
    "\n",
    "    # Save the Braille output to a text file\n",
    "    with open(\"braille_output.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(braille_output)\n",
    "\n",
    "    print(\"Conversation Turn Complete!\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 2: ASL To Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import kagglehub\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"grassknoted/asl-alphabet\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms, models\n",
    "\n",
    "def train_asl_model(\n",
    "    train_dir=\"asl_alphabet_train\",\n",
    "    num_epochs=1,\n",
    "    batch_size=120,\n",
    "    lr=0.001,\n",
    "    model_path=\"asl_model.pth\"\n",
    "):\n",
    "\n",
    "\n",
    "    # Transforms for training images\n",
    "    train_transforms = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.ToTensor(),\n",
    "        # Standard ImageNet normalization\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Load dataset from the train folder\n",
    "    # Adjust the path to your actual training images if needed\n",
    "    train_dataset = datasets.ImageFolder(\n",
    "        \"/home/varsh/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_train/asl_alphabet_train\",\n",
    "        transform=train_transforms\n",
    "    )\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "    class_names = train_dataset.classes\n",
    "    num_classes = len(class_names)\n",
    "    print(f\"Found {num_classes} classes in '{train_dir}': {class_names}\")\n",
    "\n",
    "    # Create ResNet18 with pretrained ImageNet weights\n",
    "    model = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "    \n",
    "    # Replace final layer for our number of classes\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss, correct, total = 0.0, 0, 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item() * images.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            correct += (preds == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "        epoch_loss = running_loss / total\n",
    "        epoch_acc = correct / total\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}]: \"\n",
    "              f\"Train Loss: {epoch_loss:.4f}, Train Acc: {epoch_acc:.2f}\")\n",
    "\n",
    "    # Save model weights\n",
    "    torch.save(model.state_dict(), model_path)\n",
    "    print(f\"Training complete. Model saved to '{model_path}'\")\n",
    "\n",
    "    return model, class_names\n",
    "\n",
    "def main():\n",
    "    # Call the training function\n",
    "    # Adjust the parameters as needed\n",
    "    model, class_names = train_asl_model(\n",
    "        train_dir=\"asl_alphabet_train\",\n",
    "        num_epochs=5,\n",
    "        batch_size=120,\n",
    "        lr=0.001,\n",
    "        model_path=\"asl_model.pth\"\n",
    "    )\n",
    "    print(\"Finished training.\")\n",
    "    print(\"Classes are:\", class_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "def predict_image(img_path, model, class_names):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Same transforms as training (resize, normalize)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Load and preprocess the image\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_t = transform(img).unsqueeze(0).to(device)  # shape: [1, 3, 224, 224]\n",
    "\n",
    "    # Model inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = model(img_t)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "\n",
    "    predicted_class = class_names[preds.item()]\n",
    "    return predicted_class\n",
    "\n",
    "def test_on_test_folder(test_dir, model, class_names):\n",
    "    \"\"\"\n",
    "    Iterates over all .jpg files in `test_dir`, infers their label,\n",
    "    and compares to the true label (parsed from filename).\n",
    "    \n",
    "    Example filename: \"A_test.jpg\" -> true label is \"A\".\n",
    "    Prints each result and final accuracy.\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Gather all .jpg files in the test directory\n",
    "    test_files = [f for f in os.listdir(test_dir) if f.lower().endswith(\".jpg\")]\n",
    "    total = 0\n",
    "    correct = 0\n",
    "\n",
    "    for f in test_files:\n",
    "        img_path = os.path.join(test_dir, f)\n",
    "\n",
    "        # Parse the true label from filename (before underscore)\n",
    "        # e.g. \"A_test.jpg\" -> \"A\", \"del_123.jpg\" -> \"del\"\n",
    "        true_label = f.split(\"_\")[0]\n",
    "\n",
    "        # Inference\n",
    "        predicted_label = predict_image(img_path, model, class_names)\n",
    "\n",
    "        # Compare\n",
    "        total += 1\n",
    "        if predicted_label == true_label:\n",
    "            correct += 1\n",
    "\n",
    "        print(f\"File: {f} | Pred: {predicted_label} | True: {true_label}\")\n",
    "\n",
    "    # Final accuracy\n",
    "    accuracy = (correct / total) * 100 if total > 0 else 0\n",
    "    print(f\"\\nTotal Tested: {total}, Correct: {correct}, Accuracy: {accuracy:.2f}%\")\n",
    "\n",
    "def main():\n",
    "    # 1. Recreate the exact architecture used in training\n",
    "    #    We'll assume 29 classes: A-Z + 'del', 'nothing', 'space'.\n",
    "    num_classes = 29\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Create a base ResNet18 with no pretrained weights\n",
    "    model = models.resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    # 2. Load the trained weights from file\n",
    "    state_dict = torch.load(\"asl_model.pth\", map_location=device)\n",
    "    model.load_state_dict(state_dict)\n",
    "    model.to(device)\n",
    "\n",
    "    # 3. The same class_names from your training (alphabetical order)\n",
    "    class_names = [\n",
    "        'A','B','C','D','E','F','G','H','I','J','K',\n",
    "        'L','M','N','O','P','Q','R','S','T','U','V',\n",
    "        'W','X','Y','Z','del','nothing','space'\n",
    "    ]\n",
    "\n",
    "    # 4. Test folder with single images named like \"A_test.jpg\", \"B_test.jpg\", etc.\n",
    "    test_dir = \"/home/varsh/.cache/kagglehub/datasets/grassknoted/asl-alphabet/versions/1/asl_alphabet_test/asl_alphabet_test\"\n",
    "    test_on_test_folder(test_dir, model, class_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "\n",
    "import cv2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "\n",
    "def live_demo(model, class_names):\n",
    "    \"\"\"\n",
    "    Real-time ASL sign recognition with a small bounding box (ROI).\n",
    "    The user should place their hand inside the box.\n",
    "    Press ESC to exit.\n",
    "    \"\"\"\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.eval()\n",
    "\n",
    "    # Same transforms used during training (minus random augmentation)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Resize((224, 224)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                             [0.229, 0.224, 0.225])\n",
    "    ])\n",
    "\n",
    "    # Define bounding box (ROI) coordinates and size\n",
    "    roi_x, roi_y = 50, 50   \n",
    "    roi_w, roi_h = 224, 224 \n",
    "\n",
    "    cap = cv2.VideoCapture(0)  \n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open webcam.\")\n",
    "        return\n",
    "\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # Flip horizontally for a mirror-like view\n",
    "        frame = cv2.flip(frame, 1)\n",
    "\n",
    "        # Draw the bounding box on the frame\n",
    "        cv2.rectangle(\n",
    "            frame,\n",
    "            (roi_x, roi_y),\n",
    "            (roi_x + roi_w, roi_y + roi_h),\n",
    "            (0, 255, 0), 2\n",
    "        )\n",
    "\n",
    "        # Crop the region of interest\n",
    "        roi_frame = frame[roi_y:roi_y+roi_h, roi_x:roi_x+roi_w]\n",
    "\n",
    "        # Convert ROI to RGB (OpenCV is BGR) and then to PIL\n",
    "        roi_rgb = cv2.cvtColor(roi_frame, cv2.COLOR_BGR2RGB)\n",
    "        pil_img = Image.fromarray(roi_rgb)\n",
    "\n",
    "        # Transform and infer\n",
    "        img_t = transform(pil_img).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(img_t)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "        predicted_label = class_names[preds.item()]\n",
    "\n",
    "        # Display the predicted label near the ROI\n",
    "        text_pos = (roi_x, roi_y - 10)  # slightly above the box\n",
    "        cv2.putText(frame, f\"Pred: {predicted_label}\",\n",
    "                    text_pos,\n",
    "                    cv2.FONT_HERSHEY_SIMPLEX,\n",
    "                    0.8, (0, 255, 0), 2)\n",
    "\n",
    "        # Show the frame\n",
    "        cv2.imshow(\"ASL Live Demo (ROI)\", frame)\n",
    "\n",
    "        # Press ESC to quit\n",
    "        if cv2.waitKey(1) & 0xFF == 27:\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "def main():\n",
    "    # Same class names as in training\n",
    "    class_names = [\n",
    "        'A','B','C','D','E','F','G','H','I','J','K',\n",
    "        'L','M','N','O','P','Q','R','S','T','U','V',\n",
    "        'W','X','Y','Z','del','nothing','space'\n",
    "    ]\n",
    "    num_classes = len(class_names)\n",
    "\n",
    "    # Load model\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = models.resnet18(weights=None)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "\n",
    "    # Load your trained weights\n",
    "    checkpoint = torch.load(\"asl_model.pth\", map_location=device)\n",
    "    model.load_state_dict(checkpoint)\n",
    "    model.to(device)\n",
    "\n",
    "    # Run the live ROI demo\n",
    "    live_demo(model, class_names)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Phase 3: Know Your Rights Chatbot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.embeddings import OpenAIEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "import openai\n",
    "\n",
    "# OpenAI API key\n",
    "OPENAI_API_KEY = 'Enter Your API Key Here'\n",
    "openai.api_key = OPENAI_API_KEY\n",
    "\n",
    "class PDFChatbot:\n",
    "    def __init__(self, pdf_path):\n",
    "        self.pdf_path = pdf_path\n",
    "        self.vector_db = None\n",
    "        self.qa_chain = None  \n",
    "        # Process the PDF: extract text, split it, create a vector database\n",
    "        self.load_and_vectorize_pdf()\n",
    "        # Build the QA chain using the vector database\n",
    "        self.build_qa_chain()  \n",
    "\n",
    "# Extract text from each page of the Document.\n",
    "    def extract_text(self):\n",
    "        text = \"\"\n",
    "        with pdfplumber.open(self.pdf_path) as pdf:\n",
    "            for page in pdf.pages:\n",
    "                page_text = page.extract_text()\n",
    "                if page_text:\n",
    "                    text += page_text + \"\\n\"\n",
    "        return text\n",
    "    \n",
    "# Split the extracted text into smaller chunks using a recursive character splitter.\n",
    "    def split_text(self, text):\n",
    "        splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=100)\n",
    "        return splitter.split_text(text)\n",
    "\n",
    "# Create a vector database (Chroma) from the text chunks using OpenAI embeddings.\n",
    "    def create_vector_db(self, text_chunks):\n",
    "        embeddings = OpenAIEmbeddings(openai_api_key=OPENAI_API_KEY)\n",
    "        self.vector_db = Chroma.from_texts(text_chunks, embeddings)\n",
    "\n",
    "# Extract, split, and vectorize the text.\n",
    "    def load_and_vectorize_pdf(self):\n",
    "        text = self.extract_text()\n",
    "        if not text.strip():\n",
    "            raise ValueError(\"No text could be extracted from the PDF.\")\n",
    "        text_chunks = self.split_text(text)\n",
    "        self.create_vector_db(text_chunks)\n",
    "\n",
    "# Build a RetrievalQA chain using the vector database as the retriever and OpenAI as the LLM.\n",
    "    def build_qa_chain(self):\n",
    "        retriever = self.vector_db.as_retriever()\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=OpenAI(temperature=0, openai_api_key=OPENAI_API_KEY),\n",
    "            chain_type=\"stuff\",\n",
    "            retriever=retriever\n",
    "        )\n",
    "\n",
    "    def ask(self, query):\n",
    "        full_query = f\"\"\"\n",
    "You are an AI assistant responding to a user query based on a provided document.\n",
    "Please provide a detailed answer with references from the document.\n",
    "Always include a legal disclaimer at the end: \"Disclaimer: This response is based on the provided document and is for informational purposes only. Please consult a qualified lawyer for legal advice.\"\n",
    "\n",
    "User Query: {query}\n",
    "\"\"\"\n",
    "        return self.qa_chain.run(full_query)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    pdf_path = \"Data.pdf\"\n",
    "    chatbot = PDFChatbot(pdf_path)\n",
    "    \n",
    "    while True:\n",
    "        query = input(\"Enter your query (or type 'exit' to quit): \")\n",
    "        if query.lower() == \"exit\":\n",
    "            break\n",
    "        response = chatbot.ask(query)\n",
    "        print(\"AI Response:\", response)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
